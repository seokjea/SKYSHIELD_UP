"""
17ë§Œê°œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ OpenAI Embeddingìœ¼ë¡œ chunk ë‹¨ìœ„ ì„ë² ë”©í•˜ê³ ,
ê³µê²© ë²¡í„° + HDBSCAN í´ëŸ¬ìŠ¤í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•´ ë‘ëŠ” ìŠ¤í¬ë¦½íŠ¸.

ì‚¬ìš©ë²• (backend ë””ë ‰í„°ë¦¬ì—ì„œ):

    (SKY_venv) $ python precompute_jailbreak.py \
        --embed-model "OpenAI Embedding" \
        --summ-model "OpenAI"

ì „ì œ:
- backend/.env ì— OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨.
- data/jailbreak_dataset.csv, data/jailbreak_customed.csv ê°€ ì¡´ì¬í•´ì•¼ í•¨.
"""

import argparse
import json
from pathlib import Path

import numpy as np
import pandas as pd
from dotenv import load_dotenv

from src.embedding import Embedder
from src.cluster_analyzer import ClusterAnalyzer
from src.summarizer import Summarizer
from src.utils import get_embedding_client

load_dotenv()

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
PRE_DIR = BASE_DIR / "precomputed"
VEC_DIR = PRE_DIR / "vectors"


def safe_name(s: str) -> str:
    return s.replace("/", "_").replace(" ", "_")


def load_dataset():
    """jailbreak_dataset + customed ë¥¼ í•©ì³ì„œ ê³µê²©/ì •ìƒ í…ìŠ¤íŠ¸ ë¶„ë¦¬."""
    df_main = pd.read_csv(DATA_DIR / "jailbreak_dataset.csv")
    df_custom = pd.read_csv(DATA_DIR / "jailbreak_customed.csv")
    data = pd.concat([df_main, df_custom], ignore_index=True)

    atk_mask = data["label"] == 1
    atk_texts = data.loc[atk_mask, "text"].tolist()
    norm_texts = data.loc[~atk_mask, "text"].tolist()
    return atk_texts, norm_texts


def precompute_embeddings(embed_model: str, batch_size: int = 128):
    """
    ì „ì²´ í…ìŠ¤íŠ¸(ê³µê²© + ì •ìƒ)ë¥¼ chunk ë‹¨ìœ„ë¡œ ì„ë² ë”©í•´ì„œ ë””ìŠ¤í¬ì— ì €ì¥.

    - ê³µê²© ë²¡í„°: npy (ì‘ìŒ)
    - ì •ìƒ ë²¡í„°: memmap(.dat) + meta.json
    """
    VEC_DIR.mkdir(parents=True, exist_ok=True)

    atk_texts, norm_texts = load_dataset()
    print(f"[1/3] ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  - ê³µê²© í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(atk_texts)}")
    print(f"  - ì •ìƒ í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(norm_texts)}")

    client = get_embedding_client(embed_model)
    embedder = Embedder(embed_model, client=client, batch_size=batch_size)

    # 1) ê³µê²© ë²¡í„°ëŠ” í•œ ë²ˆì— ì„ë² ë”© (ê°œìˆ˜ê°€ 1,000 ì •ë„ë¼ ë©”ëª¨ë¦¬ ì—¬ìœ )
    print("[1-1] ê³µê²© í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...")
    atk_vec = embedder.encode(atk_texts).astype("float32")
    atk_path = VEC_DIR / f"attack_{safe_name(embed_model)}.npy"
    np.save(atk_path, atk_vec)
    print(f"  - ê³µê²© ë²¡í„° ì €ì¥: {atk_path} (shape={atk_vec.shape})")

    # 2) ì •ìƒ í…ìŠ¤íŠ¸ëŠ” memmapìœ¼ë¡œ chunk ì„ë² ë”©
    print("[1-2] ì •ìƒ í…ìŠ¤íŠ¸ ì„ë² ë”© (chunk + memmap) ì¤‘...")
    n_norm = len(norm_texts)
    if n_norm == 0:
        print("  - ì •ìƒ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
        return atk_path, atk_texts

    # ì²« batch ì„ë² ë”©í•´ì„œ ì°¨ì› í™•ì¸
    first_batch = norm_texts[:batch_size]
    first_vecs = embedder.encode(first_batch).astype("float32")
    dim = first_vecs.shape[1]

    norm_path = VEC_DIR / f"normal_{safe_name(embed_model)}.dat"
    norm_mem = np.memmap(norm_path, dtype="float32", mode="w+", shape=(n_norm, dim))

    # ì²« batch ê¸°ë¡
    norm_mem[0:first_vecs.shape[0], :] = first_vecs
    idx = first_vecs.shape[0]
    print(f"  - ì²« batch ì™„ë£Œ: {idx}/{n_norm}")

    # ë‚˜ë¨¸ì§€ batch
    while idx < n_norm:
        batch_texts = norm_texts[idx: idx + batch_size]
        vecs = embedder.encode(batch_texts).astype("float32")
        end = idx + vecs.shape[0]
        norm_mem[idx:end, :] = vecs
        idx = end
        print(f"  - ì§„í–‰ ì¤‘: {idx}/{n_norm}")

    norm_mem.flush()
    del norm_mem

    # meta ì •ë³´ ì €ì¥
    meta = {"n_normals": int(n_norm), "dim": int(dim)}
    meta_path = VEC_DIR / f"normal_{safe_name(embed_model)}.meta.json"
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    print(f"  - ì •ìƒ ë²¡í„° memmap ì €ì¥: {norm_path} (n={n_norm}, dim={dim})")
    print(f"  - meta ì €ì¥: {meta_path}")

    return atk_path, atk_texts


def precompute_clusters(embed_model: str, summ_model: str, atk_vec_path: Path, atk_texts):
    """
    ê³µê²© ë²¡í„° + í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•´ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ + í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„± í›„ pklì— ì €ì¥.
    """
    PRE_DIR.mkdir(parents=True, exist_ok=True)

    print("[2/3] ê³µê²© ë²¡í„° ë¡œë“œ ì¤‘...")
    atk_vec = np.load(atk_vec_path)  # float32, shape=(n_atk, dim)
    print(f"  - atk_vec shape: {atk_vec.shape}")

    print(f"[2/3] í´ëŸ¬ìŠ¤í„°ë§ + í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„± ì¤‘... (summ_model={summ_model})")
    summarizer = Summarizer(summ_model)
    analyzer = ClusterAnalyzer(summarizer=summarizer)
    analyzer.fit(atk_vec)
    analyzer.generate_cluster_names(atk_texts, summarizer)

    fname = f"cluster_{safe_name(embed_model)}_{safe_name(summ_model)}.pkl"
    out_path = PRE_DIR / fname

    print(f"[3/3] í´ëŸ¬ìŠ¤í„° ë¶„ì„ê¸° ì €ì¥ ì¤‘... -> {out_path}")
    import pickle

    # ğŸ”¥ Summarizer(OpenAI í´ë¼ì´ì–¸íŠ¸)ëŠ” pickle ë¶ˆê°€ + ëŸ°íƒ€ì„ì—ì„œ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œê±°
    analyzer.summarizer = None

    with open(out_path, "wb") as f:
        pickle.dump(analyzer, f)

    print("ì™„ë£Œ! ì´ì œ FastAPIì—ì„œ ì‚¬ì „ ê³„ì‚°ëœ í´ëŸ¬ìŠ¤í„°ë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--embed-model", type=str, default="OpenAI Embedding")
    parser.add_argument("--summ-model", type=str, default="OpenAI")
    parser.add_argument("--batch-size", type=int, default=128)
    args = parser.parse_args()

    embed_model = args.embed_model
    summ_model = args.summ_model

    print(f"[0] embed_model={embed_model}, summ_model={summ_model}")
    atk_vec_path, atk_texts = precompute_embeddings(
        embed_model,
        batch_size=args.batch_size,
    )
    precompute_clusters(embed_model, summ_model, atk_vec_path, atk_texts)


if __name__ == "__main__":
    main()
